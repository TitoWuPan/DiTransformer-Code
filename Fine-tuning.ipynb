{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuing device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Usuing device: {device}')\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 20 18:57:01 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.22                 Driver Version: 552.22         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| 30%   29C    P8             15W /  225W |    7666MiB /   8192MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1652    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      1792    C+G   C:\\Windows\\System32\\dwm.exe                 N/A      |\n",
      "|    0   N/A  N/A      3672    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe      N/A      |\n",
      "|    0   N/A  N/A      4172      C   ...rograms\\Python\\Python311\\python.exe      N/A      |\n",
      "|    0   N/A  N/A      5028    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A      5216    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A      6352    C+G   ...a\\Local\\Programs\\Opera GX\\opera.exe      N/A      |\n",
      "|    0   N/A  N/A      6588    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      9492    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A      9616    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     10696    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     10740    C+G   ...GeForce Experience\\NVIDIA Share.exe      N/A      |\n",
      "|    0   N/A  N/A     11180    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     15628    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16208    C+G   C:\\Windows\\System32\\Taskmgr.exe             N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_caracteres(archivo, max_block_size=128):\n",
    "    with open(archivo, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    word_count = len(text.split())\n",
    "    block_size = min(word_count, max_block_size)\n",
    "    return block_size\n",
    "\n",
    "def load_dataset(file_path, tokenizer):\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = contar_caracteres(file_path),\n",
    "        cache_dir=None,\n",
    "    )\n",
    "    print(contar_caracteres(file_path))\n",
    "    return dataset\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_file_path,model_name,output_dir,overwrite_output_dir,per_device_train_batch_size,num_train_epochs):\n",
    "  \n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          logging_dir=\"C:/Users/User/DiTransformer-Code/logs\",\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT-2 Small ('gpt2'): 124 million parameters.\n",
    "## GPT-2 Medium ('gpt2-medium'): 345 million parameters.\n",
    "## GPT-2 Large ('gpt2-large'): 774 million parameters.\n",
    "## GPT-2 XL ('gpt2-xl'): 1.5 billion parameters.\n",
    "# , bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>'\n",
    "\n",
    "model_name = 'Models'\n",
    "output_dir = 'C:/Users/User/DiTransformer-Code/Models'\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 1.0\n",
    "save_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Data/Dato_Bloques/Bloque 1/Code\"\n",
    "archivos = [archivo for archivo in os.listdir(path)]\n",
    "for archivo in archivos:\n",
    "    print(archivo)\n",
    "    archivos_txt = [archivo for archivo in os.listdir(f\"{path}/{archivo}\") if archivo.endswith('.txt')]\n",
    "    for archivo_txt in archivos_txt:\n",
    "        if contar_caracteres(f\"{path}/{archivo}/{archivo_txt}\") != 0:\n",
    "            print(archivo_txt)\n",
    "            train(f\"{path}/{archivo}/{archivo_txt}\",model_name,output_dir,overwrite_output_dir,per_device_train_batch_size,num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('Models')\n",
    "model = GPT2LMHeadModel.from_pretrained('Models')\n",
    "\n",
    "def generate(code, max_length=50):\n",
    "    tokenized = tokenizer.encode(code, return_tensors='pt')\n",
    "    resp = model.generate(\n",
    "        tokenized,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "    return (tokenizer.decode(resp[0]))\n",
    "\n",
    "print(generate(\"import numpy as np\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

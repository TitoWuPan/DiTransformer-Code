{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuing device: cuda\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Usuing device: {device}')\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_caracteres(archivo, max_block_size=128):\n",
    "    with open(archivo, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    word_count = len(text.split())\n",
    "    block_size = min(word_count, max_block_size)\n",
    "    return block_size\n",
    "\n",
    "def load_dataset(file_path, tokenizer):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = contar_caracteres(file_path),\n",
    "        cache_dir=\"C:/Users/User/TP1/Cache\",\n",
    "    )\n",
    "    print(contar_caracteres(file_path))\n",
    "    return dataset\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_file_path,model_name,output_dir,overwrite_output_dir,per_device_train_batch_size,num_train_epochs):\n",
    "  \n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          logging_dir=\"C:/Users/User/TP1/logs\",\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT-2 Small ('gpt2'): 124 million parameters.\n",
    "## GPT-2 Medium ('gpt2-medium'): 345 million parameters.\n",
    "## GPT-2 Large ('gpt2-large'): 774 million parameters.\n",
    "## GPT-2 XL ('gpt2-xl'): 1.5 billion parameters.\n",
    "\n",
    "label = 1\n",
    "model_name = f'Models/Label {label}'\n",
    "output_dir = f'C:/Users/User/TP1/Models/Label {label}'\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 3\n",
    "num_train_epochs = 1.0\n",
    "save_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bloque = 1\n",
    "path = f\"Data/Dato_Bloques/Label {label}/Bloque {Bloque}/Code\"\n",
    "archivos = [archivo for archivo in os.listdir(path)]\n",
    "for archivo in archivos:\n",
    "    print(archivo)\n",
    "    archivos_txt = [archivo for archivo in os.listdir(f\"{path}/{archivo}\") if archivo.endswith('.txt')]\n",
    "    for archivo_txt in archivos_txt:\n",
    "        if contar_caracteres(f\"{path}/{archivo}/{archivo_txt}\") != 0:\n",
    "            print(archivo_txt)\n",
    "            train(f\"{path}/{archivo}/{archivo_txt}\",model_name,output_dir,overwrite_output_dir,per_device_train_batch_size,num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.preprocessing import LabelEncoder\n",
      "le = LabelEncoder()\n",
      "X[:,2] = le.fit_transform(X[:,2])\n",
      "print(X[:,0])\n",
      "\n",
      "\n",
      "Y_train=to_categorical(Y_train)\n",
      "\n",
      "\n",
      "def train_transform_data(df):\n",
      "    for col in df.columns:\n",
      "        if df[col].dtype in ('O') and df[col].dtype == 'object':\n",
      "            le.fit(df[col])\n",
      "             df[col] = le.transform(df[col])\n",
      "          return df\n",
      "    def test_transform_transform_data(df):\n",
      "            if le.fit(df[col].dtype in ('O') and df[col].dtype == 'object':\n",
      "                            and le.transform(df[col)!= 'cat':\n",
      "                                   \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# Herramientas de limpieza pyright, python lsv server\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('D:/Models/Model/Label 0')\n",
    "model = GPT2LMHeadModel.from_pretrained('D:/Models/Model/Label 0')\n",
    "\n",
    "def generate(code, max_length=300):\n",
    "    tokenized = tokenizer.encode(code, return_tensors='pt')\n",
    "    resp = model.generate(\n",
    "        tokenized,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "\n",
    "    return (tokenizer.decode(resp[0]))\n",
    "\n",
    "print(generate(\"from\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from Levenshtein import distance\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/User/CodeXGLUE\")  # Asegúrate de que este camino apunte al directorio que contiene codebleu.\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas Automáticas\n",
    "## Perplexidad:\n",
    "\n",
    "Mide cuán bien el modelo predice una muestra de texto. Una perplexidad más baja indica que el modelo es mejor para predecir el siguiente token en una secuencia. Principalmente durante el entrenamiento para monitorear el progreso.\n",
    "\n",
    "## BLEU (Bilingual Evaluation Understudy):\n",
    "\n",
    "Mide la similitud entre el código generado y uno o más códigos de referencia, basándose en la coincidencia de n-gramas. Comúnmente utilizado en la generación de texto y código para evaluar la calidad de las secuencias generadas.\n",
    "\n",
    "## METEOR (Metric for Evaluation of Translation with Explicit ORdering):\n",
    "\n",
    "Similar a BLEU pero considera la coincidencia de sinónimos y la coincidencia flexible de palabras. Evalúa la calidad del texto generado con mayor sensibilidad a la elección de palabras.\n",
    "\n",
    "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation):\n",
    "\n",
    "Mide la superposición de n-gramas, así como la superposición de palabras y la longitud de las secuencias entre el texto generado y las referencias. Frecuentemente utilizado para evaluar resúmenes automáticos y generación de texto.\n",
    "\n",
    "# Métricas Específicas para Generación de Código\n",
    "## Exact Match (EM):\n",
    "\n",
    "Mide si el código generado es exactamente igual al código de referencia. Útil para casos donde la exactitud completa es crítica.\n",
    "\n",
    "## CodeBLEU:\n",
    "\n",
    "Métrica especializada para evaluar la calidad del código generado, teniendo en cuenta aspectos específicos del código como la sintaxis, la estructura y la lógica. Especialmente útil para la evaluación de modelos de generación de código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New\n",
    "input_csv = f'Data/Dato_Bloques/Label {Label}/archivo_1_promts.csv'\n",
    "df = pd.read_csv(input_csv)\n",
    "df['generated_text'] = pd.Series(dtype=object)\n",
    "df['Exact Match'] = pd.Series(dtype=float)\n",
    "df['Perplexity'] = pd.Series(dtype=float)\n",
    "df['Bleu'] = pd.Series(dtype=float)\n",
    "df['Meteor'] = pd.Series(dtype=float)\n",
    "df['Rouge1'] = pd.Series(dtype=float)\n",
    "df['Rouge2'] = pd.Series(dtype=float)\n",
    "df['RougeL'] = pd.Series(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old\n",
    "input_csv = f'D:/Models/Logs/Label {Label}/Puntaje.csv'\n",
    "df = pd.read_csv(input_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(f'D:/Models/Model/Label {Label}')\n",
    "model = GPT2LMHeadModel.from_pretrained(f'D:/Models/Model/Label {Label}')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Evaluar segun la tarea\n",
    "def generate_text(prompt, max_length=80): # Limit 1064\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def calculate_similarity(reference, candidate):\n",
    "    max_length = max(len(reference), len(candidate))\n",
    "    similarity = (max_length - distance(reference, candidate)) / max_length * 100\n",
    "    return similarity\n",
    "\n",
    "def calculate_Perplexity(encodings):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings, labels=encodings['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    \n",
    "    return perplexity.item()\n",
    "\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores\n",
    "\n",
    "def adjust_reference(reference, target_length):\n",
    "    reference_tokens = reference.split()\n",
    "    if len(reference_tokens) > target_length:\n",
    "        return ' '.join(reference_tokens[:target_length])\n",
    "    else:\n",
    "        return reference + ' ' * (target_length - len(reference_tokens))\n",
    "    \n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(df['generated_text'][index]) and (900 - len(df['promts'][index])) < 1024:\n",
    "        prompt = row['promts']\n",
    "        reference = row['code_block']\n",
    "\n",
    "        generated_text = generate_text(prompt, (900 - len(prompt)))\n",
    "        reference = adjust_reference(reference, len(generated_text.split()))\n",
    "\n",
    "        encodings = tokenizer(generated_text, return_tensors='pt')\n",
    "    \n",
    "        accuracy = calculate_similarity(reference, generated_text)\n",
    "        Perplexity = calculate_Perplexity(encodings)\n",
    "        \n",
    "        reference_tokens = nltk.word_tokenize(reference)\n",
    "        candidate_tokens = nltk.word_tokenize(generated_text)\n",
    "\n",
    "        bleu = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "        meteor = meteor_score([reference_tokens], candidate_tokens)\n",
    "        rouge_scores = calculate_rouge(reference, generated_text)\n",
    "\n",
    "        df['generated_text'][index] = generated_text\n",
    "        df['Exact Match'][index] = accuracy\n",
    "        df['Perplexity'][index] = Perplexity\n",
    "        df['Bleu'][index] = bleu\n",
    "        df['Meteor'][index] = meteor\n",
    "        df['Rouge1'][index] = rouge_scores['rouge1'].fmeasure\n",
    "        df['Rouge2'][index] = rouge_scores['rouge2'].fmeasure\n",
    "        df['RougeL'][index] = rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "        print(f\"{index}:  Exact Match:{accuracy}  Perplexity:{Perplexity}   bleu:{bleu}   meteor:{meteor}   Rouge:{rouge_scores}\")\n",
    "\n",
    "        if index % 20 == 0:\n",
    "            df.to_csv(f'D:/Models/Logs/Label {Label}/Puntaje.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_block</th>\n",
       "      <th>promts</th>\n",
       "      <th>generated_text</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>RougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reduce_features_df.drop(columns=['days_since_p...</td>\n",
       "      <td>reduce_features_df.drop(columns=['days_since_p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X = df.drop(columns=['label'])\\nX.head()</td>\n",
       "      <td>X = df.drop(columns=['label'])\\nX.head()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_study_df = image_study_df[[\"image_id\", \"...</td>\n",
       "      <td>image_study_df = image_study_df[[\"image_id\", \"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_df.drop(columns='Path', axis=1,inplace=T...</td>\n",
       "      <td>train_df.drop(columns='Path', axis=1,inplace=T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_df.drop(columns=['Unnamed: 0'], inplace=...</td>\n",
       "      <td>train_df.drop(columns=['Unnamed: 0'], inplace=...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>study_df['PredictionString'] = study_df.progre...</td>\n",
       "      <td>study_df['PredictionString'] = study_df.progre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>df=df1[['SOPInstanceUID','PatientName','Catego...</td>\n",
       "      <td>df=df1[['SOPInstanceUID','PatientName','Catego...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>df = df[['id', 'PredictionString']]</td>\n",
       "      <td>df = df[['id', 'PredictionString']]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_im_dup = train_im_dup.drop(temp_train_im...</td>\n",
       "      <td>train_im_dup = train_im_dup.drop(temp_train_im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_im_dup = train_im_dup.reset_index(drop=T...</td>\n",
       "      <td>train_im_dup = train_im_dup.reset_index(drop=T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_im_dup = train_im_dup[[\"boxes\",\"label\",\"...</td>\n",
       "      <td>train_im_dup = train_im_dup[[\"boxes\",\"label\",\"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>opacity_df = df.dropna(subset = [\"boxes\"], inp...</td>\n",
       "      <td>opacity_df = df.dropna(subset = [\"boxes\"], inp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>del model   \\n_ = gc.collect()</td>\n",
       "      <td>del model   \\n_ = gc.collect()</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>for item in ['combined', 'matches_orig', 'matc...</td>\n",
       "      <td>for item in ['combined', 'matches_orig', 'matc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>drop_cols = ['image','image_phash','title']\\ns...</td>\n",
       "      <td>drop_cols = ['image','image_phash','title']\\ns...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>submission_df = submission_df.drop(columns='pr...</td>\n",
       "      <td>submission_df = submission_df.drop(columns='pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>del embeds</td>\n",
       "      <td>del embeds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>del x, y, train</td>\n",
       "      <td>del x, y, train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>exclude = [\\n    'date', 'weight', 'resp_1', '...</td>\n",
       "      <td>exclude = [\\n    'date', 'weight', 'resp_1', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>def features_with_tags(df):\\n    features_ds =...</td>\n",
       "      <td>def features_with_tags(df):\\n    features_ds =...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           code_block  \\\n",
       "0   reduce_features_df.drop(columns=['days_since_p...   \n",
       "1            X = df.drop(columns=['label'])\\nX.head()   \n",
       "2   image_study_df = image_study_df[[\"image_id\", \"...   \n",
       "3   train_df.drop(columns='Path', axis=1,inplace=T...   \n",
       "4   train_df.drop(columns=['Unnamed: 0'], inplace=...   \n",
       "5   study_df['PredictionString'] = study_df.progre...   \n",
       "6   df=df1[['SOPInstanceUID','PatientName','Catego...   \n",
       "7                 df = df[['id', 'PredictionString']]   \n",
       "8   train_im_dup = train_im_dup.drop(temp_train_im...   \n",
       "9   train_im_dup = train_im_dup.reset_index(drop=T...   \n",
       "10  train_im_dup = train_im_dup[[\"boxes\",\"label\",\"...   \n",
       "11  opacity_df = df.dropna(subset = [\"boxes\"], inp...   \n",
       "12                   del model   \\n_ = gc.collect()     \n",
       "13  for item in ['combined', 'matches_orig', 'matc...   \n",
       "14  drop_cols = ['image','image_phash','title']\\ns...   \n",
       "15  submission_df = submission_df.drop(columns='pr...   \n",
       "16                                         del embeds   \n",
       "17                                    del x, y, train   \n",
       "18  exclude = [\\n    'date', 'weight', 'resp_1', '...   \n",
       "19  def features_with_tags(df):\\n    features_ds =...   \n",
       "\n",
       "                                               promts generated_text  \\\n",
       "0   reduce_features_df.drop(columns=['days_since_p...            NaN   \n",
       "1            X = df.drop(columns=['label'])\\nX.head()            NaN   \n",
       "2   image_study_df = image_study_df[[\"image_id\", \"...            NaN   \n",
       "3   train_df.drop(columns='Path', axis=1,inplace=T...            NaN   \n",
       "4   train_df.drop(columns=['Unnamed: 0'], inplace=...            NaN   \n",
       "5   study_df['PredictionString'] = study_df.progre...            NaN   \n",
       "6   df=df1[['SOPInstanceUID','PatientName','Catego...            NaN   \n",
       "7                 df = df[['id', 'PredictionString']]            NaN   \n",
       "8   train_im_dup = train_im_dup.drop(temp_train_im...            NaN   \n",
       "9   train_im_dup = train_im_dup.reset_index(drop=T...            NaN   \n",
       "10  train_im_dup = train_im_dup[[\"boxes\",\"label\",\"...            NaN   \n",
       "11  opacity_df = df.dropna(subset = [\"boxes\"], inp...            NaN   \n",
       "12                   del model   \\n_ = gc.collect()              NaN   \n",
       "13  for item in ['combined', 'matches_orig', 'matc...            NaN   \n",
       "14  drop_cols = ['image','image_phash','title']\\ns...            NaN   \n",
       "15  submission_df = submission_df.drop(columns='pr...            NaN   \n",
       "16                                         del embeds            NaN   \n",
       "17                                    del x, y, train            NaN   \n",
       "18  exclude = [\\n    'date', 'weight', 'resp_1', '...            NaN   \n",
       "19  def features_with_tags(df):\\n    features_ds =...            NaN   \n",
       "\n",
       "    Exact Match  Perplexity  Bleu  Meteor  Rouge1  Rouge2  RougeL  \n",
       "0           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "1           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "2           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "3           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "4           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "5           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "6           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "7           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "8           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "9           NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "10          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "11          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "12          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "13          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "14          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "15          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "16          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "17          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "18          NaN         NaN   NaN     NaN     NaN     NaN     NaN  \n",
       "19          NaN         NaN   NaN     NaN     NaN     NaN     NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20) # 1000 cada uno - 5 mejores - testearlo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BATCH_SIZE = 32\\ntorch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\\ntorch_y_train ='"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['promts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BATCH_SIZE = 32\\ntorch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\\ntorch_y_train = torch.from_numpy(y_train).type(torch.LongTensor)\\nmodel = Model(inputs=inputs, outputs=torch_X_train)\\nmodel.compile(optimizer=Adam(0.001), loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\nmodel.summary()\\n\\n\\ndef get_validation_dataset(dataset, do_onehot=True):\\n    dataset = dataset.batch(BATCH_SIZE)\\n    dataset = dataset.prefetch(AUTO) \\n    return dataset\\ndef count_data_items(filenames):\\n     \\n   n = [int(re.compile(r\"-([0-9]*)\\\\.\").search(filename).group(1)) for filename in filenames]\") for filename in filenames]\\n      return np.sum(n)\\ndef _get_train_datas_data(filenames, do_onehot=True):\\n      \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n    \\n     \\n    \\n     \\n    \\n     \\n     \\n     \\n     \\n     \\n      \\n     \\n      \\n      \\n      \\n      \\n      \\n      \\n      \\n       \\n      \\n      \\n      \\n      \\n      \\n     \\n      \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     \\n     '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['generated_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BATCH_SIZE = 32\\ntorch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\\ntorch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) \\ntorch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\\ntorch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) \\ntrain = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\\ntest = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\\ntest_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['code_block'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'D:/Models/Logs/Label {Label}/Puntaje.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
